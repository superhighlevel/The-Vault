##### Flowcharts of classic machine learning (Left) and deep learning (Right). A convolutional neural network is used as an example for deep learning.
![[Figure 2.1.png]]
a classical [[ML]] algorithm first maps a text string to a [[vector representation]] $x$ using a set of hand-engineered features (e.g., word and character [[n-grams]], [[entities]], and phrases etc.), then learns a [[linear classifier]] with a [[softmax layer]] to compute the [[distribution]] of the domain labels $y = f(x;W)$, where $W$ is a matrix learned from training data using [[mini-batch Stochastic Gradient Descent (SGD)|SGD]] to minimize the misclassification error. The design effort is focused mainly on feature engineering

Instead of using hand-designed features for x, [[Deep Learning|DL]] approaches jointly optimize the [[feature representation]] and [[classification]] using a [[DNN]], as exemplified in Fig. 2.1 (Right). We see that the [[DNN]] consists of two halves. The top half can be viewed as a [[linear classifier]], similar to that in the classical [[ML]] model in Fig. 2.1 (Left), except that its input vector h is not based on hand-engineered features but is learned using the bottom half of the DNN, which can be viewed as a [[feature generator]] optimized jointly with the classifier in an end-to-end fashion. Unlike classical ML, the effort of designing a DL classifier is mainly on *optimizing DNN architectures for effective representation learning.*