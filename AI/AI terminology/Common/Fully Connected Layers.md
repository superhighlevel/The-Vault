#### Fully Connected layers
They perform linear projections as $W^\intercal x.^2$ We can stack multiple [[Fully Connected Layers]] to form a [[deep feed-forward NN]] by introducing a [[nonlinear activation function]] $g$ after each linear projection. If we view a text as a [[Bag-Of-Words]] and let $x$ be the sum of the [[embedding vectors]] of all words in the text, a deep FFNN can extract highly nonlinear features to represent hidden semantic topics of the text at different layers, e.g., $h^{(1)} = g (W^{(1)\intercal} x)$ at the first layer, and $h^{(2)} = g( W^{(2)\intercal}h^{(1)})$ at the second layer, and so on, where Wâ€™s are trainable matrices